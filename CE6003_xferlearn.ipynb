{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CE6003_xferlearn.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"--my1KivZOnN","colab_type":"text"},"source":["#Transfer learning\n","In this exercise we are going to use a pretrained network (VGG-16) to classify a new dataset.\n","\n","Learning Outcomes:\n","Loading & modifying keras pre-trained applications.\n","Use of Tensorflow Dataset API to setup dataset.\n","Use of built in loss functions.\n","Simple tensorflow Training Loop."]},{"cell_type":"code","metadata":{"id":"e1w9qD1lZ9Dj","colab_type":"code","outputId":"7f44a422-5b99-4aae-8072-fdac27263b4f","executionInfo":{"status":"ok","timestamp":1573730616383,"user_tz":0,"elapsed":36468,"user":{"displayName":"Tony Scanlan","photoUrl":"","userId":"11380187689366571663"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["# Install tensorflow 2.0\n","!pip install -q tensorflow==2.0.0-rc\n","import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd \n","import glob\n","import tensorflow_datasets as tfds\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","from tensorflow.keras.layers import Dense, Activation, Reshape\n","from tensorflow.keras import initializers\n","print(tf.__version__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 86.3MB 104kB/s \n","\u001b[K     |████████████████████████████████| 4.3MB 33.1MB/s \n","\u001b[K     |████████████████████████████████| 501kB 45.4MB/s \n","\u001b[?25h2.0.0-rc0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"svCLNhisabAW","colab_type":"text"},"source":["Setup Parameters for Training."]},{"cell_type":"code","metadata":{"id":"2gjpsYDXaIwE","colab_type":"code","colab":{}},"source":["H_trg =128\n","W_trg =128\n","batch_size=20\n","imagenet_rgb_values = [123.68, 116.779, 103.939]\n","target_names = ['Abyssinian','Bengal','Birman','Bombay','British Shorthair'\n","                ,'Egyptian Mau', 'Main Coon', 'Persian','Ragdoll','Russian Blue'\n","                ,'Siamese','Sphynx','American Buldog','American pit Bull'\n","                ,'Basset hound','Beagle','Boxer','Chihuahua','English Cocker Spaniel'\n","                ,'English Setter','German Shorthaired','Great Pyrenees','Havanese',\n","               'Japanese Chin','Keeshond','Leonberger','Minature Pinscher'\n","                ,'Newfoundland','Pomeranian','Pug','Saint Bernard','Samyoed'\n","               ,'Scottish Terrier','Shiba Inu','Staffordshire Bull Terrier'\n","               ,'Wheaten Terrier','Yorkshire Terrier']\n","EPOCHS = 20"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nqPiDH0LagpY","colab_type":"text"},"source":["Load Front VGG Front End. (note in parameters have set targe image size to 128x128, the vgg16 model input is nominally 224x224. Why is the change in image size OK when we are using the front end only? What implication is there for the backend because of this?) "]},{"cell_type":"code","metadata":{"id":"eaq3N55xai1L","colab_type":"code","colab":{}},"source":["\n","vgg16_model = tf.keras.applications.VGG16(weights='imagenet',include_top=False)\n","layer_name = 'block5_conv3'\n","red_model= keras.Model(inputs = vgg16_model.input, outputs=vgg16_model.get_layer(layer_name).output,trainable=False ) \n","red_model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y3xTDZDibTxE","colab_type":"text"},"source":["Build new Backend Model. It is important to setup the model to correctly interface with the front end."]},{"cell_type":"code","metadata":{"id":"FcXJ2BRxbIeh","colab_type":"code","colab":{}},"source":["def op_stage():\n","    input = keras.Input(shape=(8, 8, 512))\n","    reshape_1 = Reshape((1,8*8*512), input_shape=(8, 8,512))(input)\n","    dense_1= Dense(512, activation=\"relu\",name='dense_1',kernel_initializer=initializers.he_normal(),bias_initializer=initializers.he_normal())(reshape_1) \n","    dense_2= Dense(256, activation=\"relu\",name='dense_2',kernel_initializer=initializers.he_normal(),bias_initializer=initializers.he_normal())(dense_1) \n","    dense_3= Dense(37, activation=\"linear\",name='dense_3',kernel_initializer=initializers.he_normal(),bias_initializer=initializers.he_normal())(dense_2) \n","    act11_1  = Activation('softmax')(dense_3)\n","    \n","    model = keras.Model(inputs=input, outputs=[act11_1],trainable=True)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zYMWWx3UbtQx","colab_type":"text"},"source":["Combine the pre-trained Frontend and new backend Model (note that we only want to train the new backend part of the model during training. This can be achieved by setting layer.trainable = False as shown)"]},{"cell_type":"code","metadata":{"id":"3nIbJkWpcIwR","colab_type":"code","colab":{}},"source":["be_model = op_stage()\n","    \n","comb_model=tf.keras.Model(inputs = red_model.input, outputs= be_model(red_model.output) )\n","# modify model output for specified layers.\n","#for layer in comb_model.get_layer('model_2').layers:\n","i=0\n","for layer in comb_model.layers:\n","    if i<=17:  \n","       layer.trainable = False\n","    i+=1 \n","    \n","# Check number of Trainable parameters\n","comb_model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BCTQltwacwqe","colab_type":"text"},"source":["Use Dataset API to load dataset. Ensure to include augmentation.\n","\n","Example 37 classes: https://www.tensorflow.org/datasets/catalog/oxford_iiit_pet"]},{"cell_type":"code","metadata":{"id":"kdqripaac6g6","colab_type":"code","colab":{}},"source":["\n","def resize(input_image, height, width):\n","  input_image = tf.image.resize(input_image, [height, width],method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","  return input_image\n","\n","def random_jitter(input_image):\n","  input_image = resize(input_image, 84, 84)\n","  input_image = tf.image.random_crop(input_image, size=[ H_trg, W_trg, 3])\n","  #Random mirroring\n","  choice = tf.random.uniform(())\n","  input_image=tf.cond(choice < 0.25, lambda:  tf.image.flip_left_right(input_image), lambda: input_image)\n","  hoice = tf.random.uniform(())\n","  input_image=tf.cond(choice < 0.25, lambda:  tf.image.random_flip_up_down(input_image), lambda: input_image)\n","  hoice = tf.random.uniform(())\n","  input_image=tf.cond(choice < 0.25, lambda:  tf.image.rot90(input_image, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)), lambda: input_image)\n","  return input_image\n","     \n","def resize_test(input_image):\n","    input_image = resize(input_image, H_trg, W_trg)\n","    return input_image\n","\n","def process_image(input_img):\n","    input_img.set_shape([None, None, 3])\n","    input_img = tf.cast(input_img, tf.float32)\n","    r = input_img[:, :, 0] - imagenet_rgb_values[0]\n","    g = input_img[:, :, 1] - imagenet_rgb_values[1]\n","    b = input_img[:, :, 2] - imagenet_rgb_values[2]\n","    op_img = tf.stack([b,g,r],axis = 2)\n","    return op_img\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jpednsJlfq5l","colab_type":"code","colab":{}},"source":["import tensorflow_datasets as tfds\n","src_data,info= tfds.load(\"oxford_iiit_pet\",with_info=True)\n","print(info)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qup90biXSJvX","colab_type":"code","colab":{}},"source":["src_train_dataset = src_data[\"train\"]\n","tr_img_dataset = src_train_dataset.map(lambda x: x['image'])  \n","for img in tr_img_dataset.take(10):\n","    print(np.shape(img))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SqPWrR9MdRB_","colab_type":"code","colab":{}},"source":["def gen_datasets(src_data):    \n","\n","    src_train_dataset = src_data[\"train\"]\n","    src_test_dataset = src_data[\"test\"]\n","  \n","    # Define Training Datasets \n","    tr_img_dataset = src_train_dataset.map(lambda x: x['image'])  \n","    tr_img_dataset = tr_img_dataset.map(resize_test)\n","    #tr_img_dataset = tr_img_dataset.map(random_jitter)\n","    tr_img_dataset = tr_img_dataset.map(process_image)\n","     \n","    tr_label_dataset = src_train_dataset.map(lambda x: x['label'])  \n","\n","    \n","    # Define Test Dataset\n","    test_img_dataset = src_test_dataset.map(lambda x: x['image'])  \n","    test_img_dataset = test_img_dataset.map(resize_test)\n","    test_img_dataset = test_img_dataset.map(process_image)\n","\n","    test_label_dataset = src_test_dataset.map(lambda x: x['label']) \n"," \n","    \n","    # Join datasets.\n","    train_dataset = tf.data.Dataset.zip((tr_img_dataset, tr_label_dataset))\n","    #train_dataset=train_dataset.shuffle(3000)\n","    train_dataset = train_dataset.batch(batch_size) \n","    test_dataset = tf.data.Dataset.zip((test_img_dataset,test_label_dataset))\n","    #test_dataset=test_dataset.shuffle(250)\n","    test_dataset = test_dataset.batch(batch_size) \n","    \n","    return train_dataset,test_dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Unel7M-6deHe","colab_type":"code","colab":{}},"source":["'''\n","# Test Dataset works by displaying an image\n","tr_dataset,test_dataset = gen_datasets(src_data)\n","for img,labels in tr_dataset.take(6):\n","    model_output = comb_model(img)\n","    op_labels = tf.argmax(tf.squeeze(model_output),axis=-1)\n","    plt.figure(figsize=(6, 6))\n","    plt.imshow(img[1,:,:,:]/255.0)\n","    print(op_labels)\n","    print(labels)\n","    '''"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RywOU0bVdnXJ","colab_type":"text"},"source":["Loss functions: \n","\n","Note that can use built in loss function.\n","\n","Secondly the labels from the dataset must be converted into a on-hot representation"]},{"cell_type":"code","metadata":{"id":"3ItXgQGeeF5L","colab_type":"code","colab":{}},"source":["# Sematic label to One Hot Prediction Vector for loss function \n","def _semlabel2onehot(sem_label): \n","    sem_one_hot=tf.one_hot(sem_label,37)\n","    return sem_one_hot\n","\n","loss_object = tf.keras.losses.CategoricalCrossentropy()\n","def total_loss(target_labels_one_hot, model_output):\n","  total_loss = loss_object(target_labels_one_hot,model_output)\n","  return total_loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5MQJ9vtBfUGS","colab_type":"text"},"source":["Train step and train loop"]},{"cell_type":"code","metadata":{"id":"C4z8SZ_YfcwI","colab_type":"code","colab":{}},"source":["# optimiser for gradients\n","model_optimizer= tf.keras.optimizers.Adadelta(learning_rate=1e-2,rho=0.95,epsilon=1e-07,name='Adadelta')\n","\n","def train_step(input_image, target_labels):\n","  with tf.GradientTape() as model_tape:\n","    model_output = comb_model(input_image)\n","    # Convert input labels to one-hot format for loss function \n","    target_labels_one_hot = _semlabel2onehot(target_labels)\n","       \n","    # Loss function\n","    model_loss =  total_loss(target_labels_one_hot, tf.squeeze(model_output))\n","\n","  model_gradients = model_tape.gradient(model_loss,\n","                                          comb_model.trainable_variables)\n","\n","  model_optimizer.apply_gradients(zip(model_gradients,\n","                                          comb_model.trainable_variables))\n","\n","  return model_loss\n","\n","\n","def train(tr_dataset, test_dataset, epochs):\n","  for epoch in range(epochs):\n","    print('Epoch Number = ',epoch)\n","    counter = 0 \n","    for input_image, target_labels  in tr_dataset:\n","      model_loss = train_step(input_image, target_labels)      \n","      counter = counter+1\n","      if counter % 50 == 0:   \n","          print('loss value= ',model_loss)\n","          print(counter)\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LdqM-E6Bf0VI","colab_type":"code","colab":{}},"source":["\n","# Run training Loop\n","train(tr_dataset,test_dataset, EPOCHS)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GwLayY9mf9ME","colab_type":"text"},"source":["Verify model and obtain accuracy with Confusion matrix"]},{"cell_type":"code","metadata":{"id":"kh0pKBqCf8Sm","colab_type":"code","colab":{}},"source":["pred_labels = []\n","true_labels = []\n","counter =0\n","for input_image, target_labels  in test_dataset.take(1000):\n","     model_output = comb_model(input_image)\n","     batch_pred_labels = tf.argmax(tf.squeeze(model_output),axis=-1)\n","     pred_labels.extend(batch_pred_labels)\n","     true_labels.extend(target_labels)\n","     counter+=1\n","print(counter)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dkZHfbQ42RSP","colab_type":"code","colab":{}},"source":["#https://scikit-learn.org/0.16/auto_examples/model_selection/plot_confusion_matrix.html\n","def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(target_names))\n","    plt.xticks(tick_marks, target_names, rotation=45)\n","    plt.yticks(tick_marks, target_names)\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","\n","from sklearn.metrics import confusion_matrix\n","cm = confusion_matrix(true_labels,pred_labels)\n","plt.figure(figsize=(15, 15))\n","plot_confusion_matrix(cm)\n","\n","print('Test Set Accuracy = ',(np.cumsum(np.diag(cm))[-1])/(np.cumsum(cm))[-1])"],"execution_count":0,"outputs":[]}]}