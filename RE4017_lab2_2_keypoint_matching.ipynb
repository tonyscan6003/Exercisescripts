{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RE4017_lab2_2_keypoint_matching.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPlcvBuIhyxfuMxRh/Qjff8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonyscan6003/Exercisescripts/blob/master/RE4017_lab2_2_keypoint_matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0tPzLy0WiSm",
        "colab_type": "text"
      },
      "source": [
        "#Feature Matching with ORB\n",
        "\n",
        "In this lab we will test out feature matching with the ORB Feature Detector & Descriptor. ORB is comprised of the FAST corner detector and the BRIEF binary descriptor. \n",
        "\n",
        "![Feature Matching with ORB](https://github.com/tonyscan6003/RE4017/blob/master/images/orb_match.JPG?raw=true)\n",
        "\n",
        "Read More about ORB at:\n",
        "https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_orb/py_orb.html\n",
        "https://medium.com/analytics-vidhya/introduction-to-orb-oriented-fast-and-rotated-brief-4220e8ec40cf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7eewGJ7I-D2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import urllib\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "from mpl_toolkits import mplot3d\n",
        "from scipy import signal\n",
        "\n",
        "# Read URL Links.\n",
        "def url_to_image(url):\n",
        "\tresp = urllib.request.urlopen(url)\n",
        "\ttemp_image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "\ttemp_image = cv2.imdecode(temp_image, cv2.IMREAD_COLOR)\n",
        "\ttemp_image = cv2.cvtColor(temp_image, cv2.COLOR_BGR2RGB) # OpenCV defaults to BGR, but we need RGB here..\n",
        "\treturn temp_image\n",
        "\n",
        "# List of item urls:\n",
        "# Object\n",
        "book_url = \"https://github.com/tonyscan6003/RE4017/blob/master/images/IMG_20200228_155300.jpg?raw=true\"\n",
        "# Book test\n",
        "book_test_url1 =\"https://github.com/tonyscan6003/RE4017/blob/master/images/IMG_20200228_155346.jpg?raw=true\"\n",
        "book_test_url2 = \"https://github.com/tonyscan6003/RE4017/blob/master/images/IMG_20200228_155432.jpg?raw=true\"\n",
        "\n",
        "# Object\n",
        "house_url = \"https://github.com/tonyscan6003/RE4017/blob/master/images/IMG_20200305_144233.jpg?raw=true\"\n",
        "# House Test Images\n",
        "house_test_url2 = \"https://github.com/tonyscan6003/RE4017/blob/master/images/IMG_20200305_144436.jpg?raw=true\"\n",
        "house_test_url1 = \"https://github.com/tonyscan6003/RE4017/blob/master/images/IMG_20200305_144324.jpg?raw=true\"\n",
        "house_test_url3 = \"https://github.com/tonyscan6003/RE4017/blob/master/images/IMG_20200305_144341.jpg?raw=true\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7C-qc9Q_uKD",
        "colab_type": "text"
      },
      "source": [
        "In the cells below we will try identifying various object from a cluttered scene using feature matching. \n",
        "\n",
        "The reference image (i.e object to be found) can be changed to either of the listed items above. \n",
        "\n",
        "There is also a choice of test images to try, with the objects rearranged.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbqPw-ExBu3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ref_image_url = book_url\n",
        "test_image_url = book_test_url1\n",
        "# adjust scale factor\n",
        "scale_factor = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zer2xcpag1yO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Read in images\n",
        "ref_image = url_to_image(ref_image_url)\n",
        "test_image = url_to_image(test_image_url)\n",
        "\n",
        "# find size of images\n",
        "ref_size = np.shape(ref_image)\n",
        "test_size = np.shape(test_image) \n",
        "\n",
        "# apply scaling\n",
        "ref_image = cv2.resize(ref_image, dsize=(int(ref_size[1]/scale_factor),int(ref_size[0]/scale_factor)), interpolation=cv2.INTER_CUBIC)\n",
        "test_image = cv2.resize(test_image, dsize=(int(test_size[1]/scale_factor),int(test_size[0]/scale_factor)), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "# Plot reference and test images\n",
        "f, axarr = plt.subplots(1,2,figsize=(10,20))\n",
        "axarr[0].imshow(ref_image)\n",
        "axarr[0].axis('off') \n",
        "axarr[0].title.set_text('Reference Image')\n",
        "axarr[1].imshow(test_image)\n",
        "axarr[1].axis('off') \n",
        "axarr[1].title.set_text('Test Image')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkPIcTtATs6s",
        "colab_type": "text"
      },
      "source": [
        "#Feature Detection & Descriptor\n",
        "The ORB Descriptor includes the FAST Corner detector as its feature detector. We will display the found keypoints in the reference image, showing the keypoints with scale information and also just the keypoint locations.\n",
        "\n",
        "Given the feature detector is Corner detector, are the keypoints being found where you expect?\n",
        "\n",
        "The ORB Descriptor has been developed for use within the OpenCV python package. As can be seen in the code below it is very simple to initiate (use: ORB_create) the ORB descriptor and find the keypoints and their associated descriptors (use: detectANDCompute)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9TGPP_TUjVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert images to grey scale\n",
        "img1 = cv2.cvtColor(ref_image, cv2.COLOR_BGR2GRAY)\n",
        "img2 = cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Initiate ORB detector\n",
        "orb = cv2.ORB_create(nfeatures=1000, scoreType=cv2.ORB_FAST_SCORE)\n",
        "# find the keypoints and descriptors with ORB\n",
        "kp1, des1 = orb.detectAndCompute(img1,None)\n",
        "kp2, des2 = orb.detectAndCompute(img2,None)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocgeOuZ_EKGT",
        "colab_type": "text"
      },
      "source": [
        "Cell Below plots found keypoints in reference image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNU9t4ONWO8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "keypoints_without_size = np.copy(ref_image)\n",
        "testkp_without_size = np.copy(test_image)\n",
        "keypoints_with_size = np.copy(ref_image)\n",
        "\n",
        "cv2.drawKeypoints(ref_image, kp1, keypoints_without_size, color = (0, 255, 0))\n",
        "cv2.drawKeypoints(test_image, kp2, testkp_without_size, color = (0, 255, 0))\n",
        "cv2.drawKeypoints(ref_image, kp1, keypoints_with_size, flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "\n",
        "\n",
        "# Display image with and without keypoints size\n",
        "fx, plots = plt.subplots(1, 3, figsize=(20,10))\n",
        "\n",
        "plots[0].set_title(\"Reference Image keypoints With Scale\")\n",
        "plots[0].imshow(keypoints_with_size, cmap='gray')\n",
        "\n",
        "plots[1].set_title(\"Reference Image keypoints Location Only\")\n",
        "plots[1].imshow(keypoints_without_size, cmap='gray')\n",
        "\n",
        "plots[2].set_title(\"Test Image keypoints Location Only\")\n",
        "plots[2].imshow(testkp_without_size, cmap='gray')\n",
        "\n",
        "# Print the number of keypoints detected in the training image\n",
        "print(\"Number of Keypoints Detected In The Training Image: \", len(kp1))\n",
        "\n",
        "# Print the number of keypoints detected in the query image\n",
        "print(\"Number of Keypoints Detected In The Query Image: \", len(kp2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTT0fvlMVSdu",
        "colab_type": "text"
      },
      "source": [
        "#Perform Feature Matching\n",
        "Now that we have demonstrated finding the keypoints, we will perform matching between the keypoints of the reference and test images. The matching is performed by brute force. We will initiate the OpenCV BFMatcher and use the match command to perform matching.  Each reference image keypoint is compared against all the test image keypoints using a distance metric. The minimum distance corresponds to a match. We will dislay the best n (usually 10-20) keypoints. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBQQIeAdGWbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the number of \"Best Matches to Display\"\n",
        "n = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hzXQr3vJdro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#feature matching\n",
        "# create BFMatcher object\n",
        "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "# Match descriptors.\n",
        "matches = bf.match(des1,des2)\n",
        "# Sort them in the order of their distance.\n",
        "matches = sorted(matches, key = lambda x:x.distance)\n",
        "# Draw first n matches.\n",
        "img3 = cv2.drawMatches(img1,kp1,img2,kp2,matches[:n],None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.title('Best Matching Points')\n",
        "plt.imshow(img3),plt.show()\n",
        "\n",
        "\n",
        "print(\"\\nNumber of Matching Keypoints Between The Training and Query Images: \", len(matches))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}